{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14a13bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33dd37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OutputCodeClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "#from catboost import CatBoostClassifier\n",
    "import lime\n",
    "from lime import lime_tabular\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import precision_score,recall_score, f1_score, accuracy_score,fbeta_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import precision_score,recall_score, f1_score, accuracy_score,confusion_matrix,multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40b83ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23929, 63)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"MDS-UPDRS_Part_III-Group-A.csv\")\n",
    "df2 = pd.read_csv(\"MDS-UPDRS_Part_III-Group-B.csv\")\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faf88290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23122, 63)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(subset = ['NHY'], inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e64177bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22859, 63)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.NHY != 101.0]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a60dd6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12861"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['PDSTATE'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3333c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalised = df.replace(101.,0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "118ec40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>REC_ID</th>\n",
       "      <th>PATNO</th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>PAG_NAME</th>\n",
       "      <th>INFODT</th>\n",
       "      <th>PDTRTMNT</th>\n",
       "      <th>PDSTATE</th>\n",
       "      <th>HRPOSTMED</th>\n",
       "      <th>PDMEDYN</th>\n",
       "      <th>...</th>\n",
       "      <th>NP3RTALU</th>\n",
       "      <th>NP3RTARL</th>\n",
       "      <th>NP3RTALL</th>\n",
       "      <th>NP3RTALJ</th>\n",
       "      <th>NP3RTCON</th>\n",
       "      <th>NP3TOT</th>\n",
       "      <th>DYSKPRES</th>\n",
       "      <th>NHY</th>\n",
       "      <th>ORIG_ENTRY</th>\n",
       "      <th>LAST_UPDATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17595</td>\n",
       "      <td>671077401</td>\n",
       "      <td>57869</td>\n",
       "      <td>V04</td>\n",
       "      <td>NUPDRS3</td>\n",
       "      <td>12/2017</td>\n",
       "      <td>1.0</td>\n",
       "      <td>OFF</td>\n",
       "      <td>14.5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>01/2018</td>\n",
       "      <td>2018-01-11 15:46:02.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21551</td>\n",
       "      <td>IANT214603</td>\n",
       "      <td>111429</td>\n",
       "      <td>BL</td>\n",
       "      <td>NUPDRDOSE3</td>\n",
       "      <td>11/2022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12/2022</td>\n",
       "      <td>2022-12-20 00:00:00.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23302</td>\n",
       "      <td>IANT310558</td>\n",
       "      <td>182340</td>\n",
       "      <td>BL</td>\n",
       "      <td>NUPDRDOSE3</td>\n",
       "      <td>02/2023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>08/2023</td>\n",
       "      <td>2023-08-09 00:00:00.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20789</td>\n",
       "      <td>IAON164829</td>\n",
       "      <td>101018</td>\n",
       "      <td>V02</td>\n",
       "      <td>NUPDRDOSE3</td>\n",
       "      <td>11/2021</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ON</td>\n",
       "      <td>3.0833</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11/2021</td>\n",
       "      <td>2021-11-16 00:00:00.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16024</td>\n",
       "      <td>IANT163527</td>\n",
       "      <td>52587</td>\n",
       "      <td>V12</td>\n",
       "      <td>NUPDRDOSE3</td>\n",
       "      <td>04/2021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>05/2021</td>\n",
       "      <td>2021-05-04 00:00:00.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11959</th>\n",
       "      <td>2094</td>\n",
       "      <td>402071901</td>\n",
       "      <td>3203</td>\n",
       "      <td>V07</td>\n",
       "      <td>NUPDRS3</td>\n",
       "      <td>06/2013</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ON</td>\n",
       "      <td>1.7500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>06/2013</td>\n",
       "      <td>2013-06-19 10:32:36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11961</th>\n",
       "      <td>720</td>\n",
       "      <td>532644601</td>\n",
       "      <td>3069</td>\n",
       "      <td>V10</td>\n",
       "      <td>NUPDRS3</td>\n",
       "      <td>08/2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>08/2015</td>\n",
       "      <td>2020-06-24 12:34:50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11962</th>\n",
       "      <td>22646</td>\n",
       "      <td>IANT165575</td>\n",
       "      <td>153027</td>\n",
       "      <td>BL</td>\n",
       "      <td>NUPDRDOSE3</td>\n",
       "      <td>08/2022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>09/2022</td>\n",
       "      <td>2022-09-01 00:00:00.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11963</th>\n",
       "      <td>18553</td>\n",
       "      <td>478056701</td>\n",
       "      <td>60043</td>\n",
       "      <td>V02</td>\n",
       "      <td>NUPDRS3</td>\n",
       "      <td>11/2014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11/2014</td>\n",
       "      <td>2020-01-24 15:33:29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11964</th>\n",
       "      <td>3195</td>\n",
       "      <td>IANT162050</td>\n",
       "      <td>3318</td>\n",
       "      <td>R16</td>\n",
       "      <td>NUPDRDOSE3</td>\n",
       "      <td>02/2022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>02/2022</td>\n",
       "      <td>2022-02-09 00:00:00.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22859 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0      REC_ID   PATNO EVENT_ID    PAG_NAME   INFODT  PDTRTMNT  \\\n",
       "0           17595   671077401   57869      V04     NUPDRS3  12/2017       1.0   \n",
       "1           21551  IANT214603  111429       BL  NUPDRDOSE3  11/2022       0.0   \n",
       "2           23302  IANT310558  182340       BL  NUPDRDOSE3  02/2023       0.0   \n",
       "3           20789  IAON164829  101018      V02  NUPDRDOSE3  11/2021       1.0   \n",
       "4           16024  IANT163527   52587      V12  NUPDRDOSE3  04/2021       0.0   \n",
       "...           ...         ...     ...      ...         ...      ...       ...   \n",
       "11959        2094   402071901    3203      V07     NUPDRS3  06/2013       1.0   \n",
       "11961         720   532644601    3069      V10     NUPDRS3  08/2015       NaN   \n",
       "11962       22646  IANT165575  153027       BL  NUPDRDOSE3  08/2022       0.0   \n",
       "11963       18553   478056701   60043      V02     NUPDRS3  11/2014       0.0   \n",
       "11964        3195  IANT162050    3318      R16  NUPDRDOSE3  02/2022       0.0   \n",
       "\n",
       "      PDSTATE  HRPOSTMED  PDMEDYN  ...  NP3RTALU NP3RTARL NP3RTALL NP3RTALJ  \\\n",
       "0         OFF    14.5000      1.0  ...       1.0      2.0      3.0      0.0   \n",
       "1         NaN        NaN      0.0  ...       0.0      0.0      0.0      0.0   \n",
       "2         NaN        NaN      0.0  ...       1.0      0.0      0.0      0.0   \n",
       "3          ON     3.0833      1.0  ...       0.0      0.0      0.0      0.0   \n",
       "4         NaN        NaN      0.0  ...       0.0      0.0      0.0      0.0   \n",
       "...       ...        ...      ...  ...       ...      ...      ...      ...   \n",
       "11959      ON     1.7500      1.0  ...       2.0      0.0      0.0      0.0   \n",
       "11961     NaN        NaN      NaN  ...       0.0      0.0      0.0      0.0   \n",
       "11962     NaN        NaN      NaN  ...       0.0      0.0      0.0      0.0   \n",
       "11963     NaN        NaN      0.0  ...       0.0      0.0      0.0      0.0   \n",
       "11964     NaN        NaN      NaN  ...       0.0      0.0      0.0      0.0   \n",
       "\n",
       "      NP3RTCON  NP3TOT  DYSKPRES  NHY  ORIG_ENTRY            LAST_UPDATE  \n",
       "0          0.0    54.0       0.0  2.0     01/2018  2018-01-11 15:46:02.0  \n",
       "1          0.0     0.0       0.0  0.0     12/2022  2022-12-20 00:00:00.0  \n",
       "2          1.0    16.0       0.0  1.0     08/2023  2023-08-09 00:00:00.0  \n",
       "3          0.0    15.0       0.0  2.0     11/2021  2021-11-16 00:00:00.0  \n",
       "4          0.0     0.0       0.0  0.0     05/2021  2021-05-04 00:00:00.0  \n",
       "...        ...     ...       ...  ...         ...                    ...  \n",
       "11959      3.0    26.0       1.0  1.0     06/2013  2013-06-19 10:32:36.0  \n",
       "11961      0.0     1.0       0.0  0.0     08/2015  2020-06-24 12:34:50.0  \n",
       "11962      0.0    18.0       0.0  1.0     09/2022  2022-09-01 00:00:00.0  \n",
       "11963      0.0     0.0       0.0  0.0     11/2014  2020-01-24 15:33:29.0  \n",
       "11964      0.0     NaN       0.0  2.0     02/2022  2022-02-09 00:00:00.0  \n",
       "\n",
       "[22859 rows x 53 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 70  # Set the threshold for missing values percentage\n",
    "\n",
    "# Calculate the percentage of missing values for each column\n",
    "null_values = df_normalised.isna().mean() * 100\n",
    "\n",
    "# Identify columns with missing values greater than the threshold\n",
    "columns_to_drop = null_values[null_values > threshold].index\n",
    "\n",
    "# Drop the identified columns\n",
    "df_normalised = df_normalised.drop(columns=columns_to_drop)\n",
    "\n",
    "# Display the DataFrame after dropping columns\n",
    "df_normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe682ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns with missing values\n",
    "numerical_columns_with_missing = df_normalised.select_dtypes(include='number').columns[df_normalised.select_dtypes(include='number').isnull().any()]\n",
    "\n",
    "# Replace missing values with median for each numerical column\n",
    "for column in numerical_columns_with_missing:\n",
    "    median_value = df_normalised[column].median()  #Try using mode to see the change \n",
    "    df_normalised[column].fillna(median_value, inplace=True)\n",
    "\n",
    "    \n",
    "# Filling the null values with median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b972a692",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_normalised.drop(['REC_ID','HRPOSTMED','PDMEDTM','EVENT_ID','Unnamed: 0','PDMEDDT','EXAMDT','PDSTATE','EXAMTM','INFODT', 'PDTRTMNT','ORIG_ENTRY', 'LAST_UPDATE'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "520dbe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.get_dummies(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4789391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PATNO', 'PDMEDYN', 'DBSYN', 'NP3SPCH', 'NP3FACXP', 'NP3RIGN',\n",
       "       'NP3RIGRU', 'NP3RIGLU', 'NP3RIGRL', 'NP3RIGLL', 'NP3FTAPR', 'NP3FTAPL',\n",
       "       'NP3HMOVR', 'NP3HMOVL', 'NP3PRSPR', 'NP3PRSPL', 'NP3TTAPR', 'NP3TTAPL',\n",
       "       'NP3LGAGR', 'NP3LGAGL', 'NP3RISNG', 'NP3GAIT', 'NP3FRZGT', 'NP3PSTBL',\n",
       "       'NP3POSTR', 'NP3BRADY', 'NP3PTRMR', 'NP3PTRML', 'NP3KTRMR', 'NP3KTRML',\n",
       "       'NP3RTARU', 'NP3RTALU', 'NP3RTARL', 'NP3RTALL', 'NP3RTALJ', 'NP3RTCON',\n",
       "       'NP3TOT', 'DYSKPRES', 'NHY', 'PAG_NAME_NUPDR3OF', 'PAG_NAME_NUPDR3ON',\n",
       "       'PAG_NAME_NUPDRDOSE3', 'PAG_NAME_NUPDRS3', 'PAG_NAME_NUPDRS3A'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4104279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    10404\n",
       "0.0     7861\n",
       "1.0     3394\n",
       "3.0      967\n",
       "4.0      173\n",
       "5.0       60\n",
       "Name: NHY, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new['NHY'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fbb0ed",
   "metadata": {},
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "resample=SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))\n",
    "x_train, y_train = resample.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d83d493",
   "metadata": {},
   "source": [
    "y_train.value_counts() # Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73163ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df_new, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66e8a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=train.drop('NHY',axis=1)\n",
    "y_train=train['NHY']\n",
    "\n",
    "x_test=test.drop('NHY',axis=1)\n",
    "y_test=test['NHY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1f46cc",
   "metadata": {},
   "source": [
    "# Model Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "2b851f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:20:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:22:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:22:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "model1 = DecisionTreeClassifier()\n",
    "model2 = KNeighborsClassifier()\n",
    "model3 = LogisticRegression()\n",
    "model4 = GradientBoostingClassifier(learning_rate = 0.1, max_depth = 7, n_estimators = 60, min_samples_split = 4,min_samples_leaf=3,subsample=1)\n",
    "model5 = GaussianNB()\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "dt = DecisionTreeClassifier(max_depth = 7,min_samples_split = 4,min_samples_leaf=3)\n",
    "model6 = AdaBoostClassifier(n_estimators=80, base_estimator=dt,learning_rate=1)\n",
    "#--------------------------------\n",
    "model7 = XGBClassifier(colsample_bytree =  0.9, learning_rate = 0.2, max_depth = 7, min_child_weight =  1, n_estimators = 300, subsample = 0.9)\n",
    "model8 = BaggingClassifier(base_estimator=model6, n_estimators=60,random_state=1, n_jobs=5)\n",
    "#--------------------------------\n",
    "layer1 = [GradientBoostingClassifier(random_state=0,learning_rate = 0.1, max_depth = 7, n_estimators = 60, min_samples_split = 4,min_samples_leaf=3,subsample=1)]\n",
    "layer2 = XGBClassifier(random_state=0,colsample_bytree =  0.9, learning_rate = 0.2, max_depth = 5, min_child_weight =  1, n_estimators = 300, subsample = 0.9)\n",
    "model9 = StackingClassifier(classifiers=layer1, meta_classifier=layer2)\n",
    "#--------------------------------\n",
    "gbc = GradientBoostingClassifier(random_state=0)\n",
    "xgb = XGBClassifier(random_state=0)\n",
    "dct = DecisionTreeClassifier(random_state=0)\n",
    "acb = AdaBoostClassifier(random_state=0)\n",
    "model10 = VotingClassifier(estimators = [('GBC', gbc), ('XGB', xgb),('acb', acb)], voting = 'soft')\n",
    "#--------------------------------\n",
    "model11 = RandomForestClassifier(max_depth= 20, n_estimators= 500)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "model1.fit(x_train,y_train)\n",
    "model2.fit(x_train,y_train)\n",
    "model3.fit(x_train,y_train)\n",
    "model4.fit(x_train,y_train)\n",
    "model5.fit(x_train,y_train)\n",
    "model6.fit(x_train,y_train)\n",
    "model7.fit(x_train,y_train)\n",
    "model8.fit(x_train,y_train)\n",
    "model9.fit(x_train,y_train)\n",
    "model10.fit(x_train,y_train)\n",
    "model11.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "pred1=model1.predict(x_test)\n",
    "pred2=model2.predict(x_test)\n",
    "pred3=model3.predict(x_test)\n",
    "pred4=model4.predict(x_test)\n",
    "pred5=model5.predict(x_test)\n",
    "pred6=model6.predict(x_test)\n",
    "pred7=model7.predict(x_test)\n",
    "pred8=model8.predict(x_test)\n",
    "pred9=model9.predict(x_test)\n",
    "pred10=model10.predict(x_test)\n",
    "pred11=model11.predict(x_test)\n",
    "\n",
    "\n",
    "pred1t=model1.predict(x_train)\n",
    "pred2t=model2.predict(x_train)\n",
    "pred3t=model3.predict(x_train)\n",
    "pred4t=model4.predict(x_train)\n",
    "pred5t=model5.predict(x_train)\n",
    "pred6t=model6.predict(x_train)\n",
    "pred7t=model7.predict(x_train)\n",
    "pred8t=model8.predict(x_train)\n",
    "pred9t=model9.predict(x_train)\n",
    "pred10t=model10.predict(x_train) \n",
    "pred11t=model11.predict(x_train) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be73fb43",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "e89c4e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score:  0.8326820889647997\n",
      "Precision:  0.9186985950281267\n",
      "Recall:  0.793336519646405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x23928850e10>"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_labels = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "cmap = \"Blues\"\n",
    "colorbar = False\n",
    "values_format = \".3f\"\n",
    "title_size = 16\n",
    "\n",
    "\n",
    "rf_cm = confusion_matrix(y_test, pred11)\n",
    "print(\"F1-score: \",f1_score(y_test, pred11, average=\"macro\"))\n",
    "print(\"Precision: \",precision_score(y_test, pred11, average=\"macro\"))\n",
    "print(\"Recall: \",recall_score(y_test, pred11, average=\"macro\"))\n",
    "ConfusionMatrixDisplay(confusion_matrix=rf_cm, display_labels=display_labels).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "ac30b6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score:  0.8953843412381944\n",
      "Precision:  0.9456478582145703\n",
      "Recall:  0.8677815879144384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x239249ba7f0>"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb_cm = confusion_matrix(y_test, pred7)\n",
    "print(\"F1-score: \",f1_score(y_test, pred7, average=\"macro\"))\n",
    "print(\"Precision: \",precision_score(y_test, pred7, average=\"macro\"))\n",
    "print(\"Recall: \",recall_score(y_test, pred7, average=\"macro\"))\n",
    "ConfusionMatrixDisplay(confusion_matrix=xgb_cm, display_labels=display_labels).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "f95a88f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score:  0.8386392936351191\n",
      "Precision:  0.8784073983504591\n",
      "Recall:  0.814261386444898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x23944e21748>"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc_cm = confusion_matrix(y_test, pred9)\n",
    "print(\"F1-score: \",f1_score(y_test, pred9, average=\"macro\"))\n",
    "print(\"Precision: \",precision_score(y_test, pred9, average=\"macro\"))\n",
    "print(\"Recall: \",recall_score(y_test, pred9, average=\"macro\"))\n",
    "ConfusionMatrixDisplay(confusion_matrix=sc_cm, display_labels=display_labels).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94491f52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "f412071c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score:  0.9951326406421206\n",
      "Precision:  0.998187522771039\n",
      "Recall:  0.9921439228406913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x23944e22eb8>"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rft_cm = confusion_matrix(y_train, pred11t)\n",
    "print(\"F1-score: \",f1_score(y_train, pred11t, average=\"macro\"))\n",
    "print(\"Precision: \",precision_score(y_train, pred11t, average=\"macro\"))\n",
    "print(\"Recall: \",recall_score(y_train, pred11t, average=\"macro\"))\n",
    "ConfusionMatrixDisplay(confusion_matrix=rft_cm, display_labels=display_labels).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "17b5c1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score:  0.9995345855443739\n",
      "Precision:  0.9996023903632599\n",
      "Recall:  0.9994671145120765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x23944f07208>"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgbt_cm = confusion_matrix(y_train, pred7t)\n",
    "print(\"F1-score: \",f1_score(y_train, pred7t, average=\"macro\"))\n",
    "print(\"Precision: \",precision_score(y_train, pred7t, average=\"macro\"))\n",
    "print(\"Recall: \",recall_score(y_train, pred7t, average=\"macro\"))\n",
    "ConfusionMatrixDisplay(confusion_matrix=xgbt_cm, display_labels=display_labels).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93351965",
   "metadata": {},
   "source": [
    "# Testing scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "89c062a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090113735783028\n",
      "0.8346456692913385\n",
      "0.6977252843394576\n",
      "0.931758530183727\n",
      "0.7077865266841645\n",
      "0.8700787401574803\n",
      "0.947069116360455\n",
      "0.9111986001749781\n",
      "0.9313210848643919\n",
      "0.9203849518810149\n",
      "0.9387576552930884\n"
     ]
    }
   ],
   "source": [
    "print(model1.score(x_test,y_test))\n",
    "print(model2.score(x_test,y_test))\n",
    "print(model3.score(x_test,y_test))\n",
    "print(model4.score(x_test,y_test))\n",
    "print(model5.score(x_test,y_test))\n",
    "print(model6.score(x_test,y_test))\n",
    "print(model7.score(x_test,y_test))\n",
    "print(model8.score(x_test,y_test))\n",
    "print(model9.score(x_test,y_test))\n",
    "print(model10.score(x_test,y_test))\n",
    "print(model11.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71e6656",
   "metadata": {},
   "source": [
    "# Training scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecfd07dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.9997083556117241\n",
      "2 0.8802313712146989\n",
      "3 0.6753025810528362\n",
      "4 0.9760365527633306\n",
      "5 0.6895931560783551\n",
      "6 0.9001117970155058\n",
      "7 0.9893549798279299\n",
      "8 0.9467748991396491\n",
      "9 0.9758907305691926\n",
      "10 0.9534827200699947\n",
      "11 0.9962572303504593\n"
     ]
    }
   ],
   "source": [
    "print(\"1\",model1.score(x_train,y_train))\n",
    "print(\"2\",model2.score(x_train,y_train))\n",
    "print(\"3\",model3.score(x_train,y_train))\n",
    "print(\"4\",model4.score(x_train,y_train))\n",
    "print(\"5\",model5.score(x_train,y_train))\n",
    "print(\"6\",model6.score(x_train,y_train))\n",
    "print(\"7\",model7.score(x_train,y_train))\n",
    "print(\"8\",model8.score(x_train,y_train))\n",
    "print(\"9\",model9.score(x_train,y_train))\n",
    "print(\"10\",model10.score(x_train,y_train))\n",
    "print(\"11\",model11.score(x_train,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31666382",
   "metadata": {},
   "source": [
    "# F1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebe00f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.8367611515815586\n",
      "2 0.6229940135559441\n",
      "3 0.2576186015927868\n",
      "4 0.8305621367347958\n",
      "5 0.4014828557660956\n",
      "6 0.8187763798508157\n",
      "7 0.8971239142082089\n",
      "8 0.8150559183267302\n",
      "9 0.8386392936351191\n",
      "10 0.8455384016494906\n",
      "11 0.8413199513711658\n"
     ]
    }
   ],
   "source": [
    "print(\"1\",f1_score(y_test, model1.predict(x_test),average=\"macro\"))\n",
    "print(\"2\",f1_score(y_test, model2.predict(x_test),average='macro'))\n",
    "print(\"3\",f1_score(y_test, model3.predict(x_test),average='macro'))\n",
    "print(\"4\",f1_score(y_test, model4.predict(x_test),average='macro'))\n",
    "print(\"5\",f1_score(y_test, model5.predict(x_test),average='macro'))\n",
    "print(\"6\",f1_score(y_test, model6.predict(x_test),average='macro'))\n",
    "print(\"7\",f1_score(y_test, model7.predict(x_test),average='macro'))\n",
    "print(\"8\",f1_score(y_test, model8.predict(x_test),average='macro'))\n",
    "print(\"9\",f1_score(y_test, model9.predict(x_test),average='macro'))\n",
    "print(\"10\",f1_score(y_test, model10.predict(x_test),average='macro'))\n",
    "print(\"11\",f1_score(y_test, model11.predict(x_test),average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "9becd814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p10</th>\n",
       "      <th>p4</th>\n",
       "      <th>p7</th>\n",
       "      <th>p9</th>\n",
       "      <th>p11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   p10   p4   p7   p9  p11\n",
       "0  0.0  0.0  0.0  0.0  0.0\n",
       "1  0.0  0.0  0.0  0.0  0.0\n",
       "2  2.0  2.0  2.0  2.0  2.0\n",
       "3  0.0  0.0  0.0  0.0  0.0\n",
       "4  2.0  2.0  2.0  2.0  2.0\n",
       "5  0.0  0.0  0.0  0.0  0.0\n",
       "6  2.0  2.0  2.0  2.0  2.0\n",
       "7  3.0  3.0  3.0  3.0  3.0\n",
       "8  2.0  2.0  2.0  2.0  2.0\n",
       "9  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_df = pd.DataFrame({'p10': pred10,'p4': pred4, 'p7': pred7,'p9': pred9,'p11': pred11})\n",
    "p_df_t = pd.DataFrame({'p10': pred10t,'p4': pred4t,'p7': pred7t,'p9': pred9t,'p11': pred11t})\n",
    "p_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "aac5e5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:11:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.8, max_delta_step=0,\n",
       "              max_depth=3, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=110, n_jobs=16,\n",
       "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
       "              subsample=0.3, tree_method='exact', validate_parameters=1,\n",
       "              verbosity=None)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_model =  XGBClassifier(learning_rate= 0.8, max_depth= 3, subsample= 0.3,n_estimators = 110)\n",
    "#meta_model = RandomForestClassifier(max_depth= 7, n_estimators= 100)\n",
    "meta_model.fit(p_df_t,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "09420285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9453193350831146"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_model.score(p_df,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "1113801b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MM f1 score 0.8519024614185535\n"
     ]
    }
   ],
   "source": [
    "p_df_pred = meta_model.predict(p_df)\n",
    "accuracy_score(y_test,p_df_pred)\n",
    "\n",
    "print(\"MM f1 score\",f1_score(y_test, meta_model.predict(p_df),average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "a94a4362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9746281714785652"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(p_df_pred,p_df['p11'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0675c100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score:  0.8964399731104201\n",
      "Precision:  0.9407149268788707\n",
      "Recall:  0.8700876313014044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x2391e0ce860>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mm_cm = confusion_matrix(y_test, p_df_pred)\n",
    "print(\"F1-score: \",f1_score(y_test, p_df_pred, average=\"macro\"))\n",
    "print(\"Precision: \",precision_score(y_test, p_df_pred, average=\"macro\"))\n",
    "print(\"Recall: \",recall_score(y_test, p_df_pred, average=\"macro\"))\n",
    "ConfusionMatrixDisplay(confusion_matrix=mm_cm, display_labels=display_labels).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959c6305",
   "metadata": {},
   "source": [
    "# Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b2f0880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:51:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:51:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:51:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:51:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:51:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:51:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:51:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:51:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:51:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:51:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:51:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:51:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:51:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best set of hyperparameters:  {'learning_rate': 0.1, 'max_depth': 7, 'subsample': 1}\n",
      "Best score:  0.9976182209224229\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'subsample': [0.5, 0.7, 1]\n",
    "}\n",
    "\n",
    "# Create the XGBoost model object\n",
    "xgb_model = XGBClassifier()\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(p_df_t, y_train)\n",
    "\n",
    "# Print the best set of hyperparameters and the corresponding score\n",
    "print(\"Best set of hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811dff19",
   "metadata": {},
   "source": [
    "# Mode checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "da4ad7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traning set\n",
    "most_frequent_values_train = p_df_t.mode(axis=1).iloc[:, 0]\n",
    "voted_df_train = pd.DataFrame(most_frequent_values_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "2c926e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Set\n",
    "most_frequent_values_test = p_df.mode(axis=1).iloc[:, 0]\n",
    "voted_df_test = pd.DataFrame(most_frequent_values_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "76a82cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9343832020997376"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,voted_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "84674aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_p_df = p_df\n",
    "new_p_df['mm'] = p_df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "bd64c562",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df_list = []\n",
    "for i in range(len(new_p_df)):\n",
    "    freq = new_p_df.loc[i,:].mode()[0]\n",
    "    p_df_list.append(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "c3c08270",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df_list = pd.DataFrame(p_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "2ca952d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9391951006124234"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,p_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "8bb4cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_df = p_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "548d3253",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_df['mm'] = p_df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "2b968e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = []\n",
    "for i in range(len(mp_df)):\n",
    "    freq = mp_df.iloc[i][0]\n",
    "    mode.append(freq)\n",
    "    freq2 = y_test.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "f04dc333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "3.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 2.0\n",
      "5.0 5.0\n",
      "1.0 1.0\n",
      "3.0 3.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "1.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 3.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "3.0 3.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 1.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "0.0 0.0\n",
      "0.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "5.0 5.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "2.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "1.0 2.0\n",
      "0.0 0.0\n",
      "3.0 3.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 3.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "4.0 4.0\n",
      "0.0 0.0\n",
      "0.0 1.0\n",
      "2.0 1.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "2.0 4.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 1.0\n",
      "1.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "3.0 3.0\n",
      "3.0 3.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 1.0\n",
      "1.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 1.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 3.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "2.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 1.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 1.0\n",
      "2.0 2.0\n",
      "1.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "4.0 5.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 3.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "3.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 3.0\n",
      "3.0 3.0\n",
      "5.0 5.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 3.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 3.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 0.0\n",
      "0.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 1.0\n",
      "2.0 2.0\n",
      "1.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "3.0 3.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 1.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "1.0 0.0\n",
      "2.0 2.0\n",
      "4.0 4.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "1.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 1.0\n",
      "1.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "3.0 4.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 0.0\n",
      "3.0 3.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "0.0 0.0\n",
      "1.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "4.0 4.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "3.0 4.0\n",
      "2.0 2.0\n",
      "3.0 4.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "1.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "5.0 5.0\n",
      "1.0 1.0\n",
      "2.0 1.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "4.0 4.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "3.0 3.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 1.0\n",
      "2.0 2.0\n",
      "3.0 4.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "3.0 3.0\n",
      "0.0 0.0\n",
      "5.0 5.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 3.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "4.0 4.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "3.0 3.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "3.0 3.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 1.0\n",
      "0.0 0.0\n",
      "2.0 3.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "0.0 2.0\n",
      "0.0 1.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "3.0 3.0\n",
      "0.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "3.0 2.0\n",
      "3.0 3.0\n",
      "1.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "1.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "4.0 4.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "3.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "0.0 0.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 1.0\n",
      "0.0 0.0\n",
      "2.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "1.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 3.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 3.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "3.0 4.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "3.0 3.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "3.0 4.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 1.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 3.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "3.0 3.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 1.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "3.0 3.0\n",
      "1.0 1.0\n",
      "4.0 4.0\n",
      "2.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "4.0 4.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 3.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "3.0 4.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "4.0 4.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "3.0 4.0\n",
      "3.0 3.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "4.0 4.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "3.0 3.0\n",
      "1.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "3.0 4.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 2.0\n",
      "0.0 0.0\n",
      "2.0 3.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "1.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "3.0 3.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "4.0 4.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "3.0 3.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "3.0 3.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 1.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "2.0 2.0\n",
      "1.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "0.0 0.0\n",
      "3.0 4.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n",
      "2.0 2.0\n"
     ]
    }
   ],
   "source": [
    "mode = []\n",
    "for i in range(len(p_df_list)):\n",
    "    freq = p_df_list.iloc[i][0]\n",
    "    mode.append(freq)\n",
    "    freq2 = y_test.iloc[i]\n",
    "    print(freq,freq2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e930df",
   "metadata": {},
   "source": [
    "# Duplicates checking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328446fe",
   "metadata": {},
   "source": [
    "Comparning models predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "2af4bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dup = p_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "4ce0fe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_list = []\n",
    "for i in range(len(y_test)):\n",
    "    y_list.append(y_test.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "3ec52e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dup['label'] = y_list\n",
    "new_dup['mode'] = mode\n",
    "new_dup['mm'] = p_df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "119413a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9453193350831146"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,new_dup['mm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c610053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "0e9c3de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gbc</th>\n",
       "      <th>adb</th>\n",
       "      <th>xgb</th>\n",
       "      <th>sc</th>\n",
       "      <th>rf</th>\n",
       "      <th>mm</th>\n",
       "      <th>label</th>\n",
       "      <th>mode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gbc  adb  xgb   sc   rf   mm  label  mode\n",
       "0   0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0\n",
       "1   0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0\n",
       "2   2.0  2.0  2.0  2.0  2.0  2.0    2.0   2.0\n",
       "3   0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0\n",
       "4   2.0  2.0  2.0  2.0  2.0  2.0    2.0   2.0\n",
       "5   0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0\n",
       "6   2.0  2.0  2.0  2.0  2.0  2.0    2.0   2.0\n",
       "7   3.0  3.0  3.0  3.0  3.0  3.0    3.0   3.0\n",
       "8   2.0  2.0  2.0  2.0  2.0  2.0    2.0   2.0\n",
       "9   0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0\n",
       "10  1.0  1.0  1.0  1.0  1.0  1.0    1.0   1.0\n",
       "11  2.0  2.0  2.0  2.0  2.0  2.0    2.0   2.0\n",
       "12  2.0  2.0  2.0  2.0  2.0  2.0    2.0   2.0\n",
       "13  0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0\n",
       "14  2.0  2.0  2.0  2.0  2.0  2.0    2.0   2.0\n",
       "15  0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0\n",
       "16  0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0\n",
       "17  2.0  2.0  2.0  2.0  2.0  2.0    2.0   2.0\n",
       "18  2.0  2.0  3.0  2.0  2.0  3.0    2.0   2.0\n",
       "19  2.0  2.0  2.0  2.0  2.0  2.0    2.0   2.0\n",
       "20  2.0  2.0  2.0  2.0  2.0  2.0    2.0   2.0\n",
       "21  2.0  1.0  2.0  2.0  2.0  2.0    1.0   2.0\n",
       "22  0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0\n",
       "23  2.0  2.0  2.0  2.0  2.0  2.0    2.0   2.0\n",
       "24  2.0  2.0  2.0  2.0  2.0  2.0    2.0   2.0\n",
       "25  0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0\n",
       "26  0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0\n",
       "27  2.0  2.0  2.0  2.0  2.0  2.0    2.0   2.0\n",
       "28  1.0  2.0  2.0  2.0  2.0  2.0    2.0   2.0\n",
       "29  3.0  3.0  3.0  3.0  2.0  3.0    2.0   3.0\n",
       "30  0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0\n",
       "31  0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0\n",
       "32  0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0\n",
       "33  0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0\n",
       "34  1.0  2.0  1.0  1.0  1.0  1.0    1.0   1.0\n",
       "35  0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0\n",
       "36  0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0\n",
       "37  1.0  1.0  1.0  1.0  1.0  1.0    1.0   1.0\n",
       "38  3.0  3.0  3.0  3.0  3.0  3.0    3.0   3.0\n",
       "39  2.0  2.0  2.0  2.0  2.0  2.0    2.0   2.0\n",
       "40  2.0  2.0  2.0  2.0  2.0  2.0    2.0   2.0\n",
       "41  0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0\n",
       "42  2.0  2.0  2.0  2.0  2.0  2.0    2.0   2.0\n",
       "43  2.0  2.0  2.0  2.0  2.0  2.0    2.0   2.0\n",
       "44  0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0\n",
       "45  2.0  1.0  2.0  2.0  2.0  2.0    1.0   2.0\n",
       "46  2.0  2.0  2.0  2.0  2.0  2.0    2.0   2.0\n",
       "47  0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0\n",
       "48  2.0  2.0  2.0  2.0  2.0  2.0    2.0   2.0\n",
       "49  0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0\n",
       "50  2.0  2.0  2.0  2.0  2.0  2.0    2.0   2.0\n",
       "51  0.0  0.0  0.0  0.0  0.0  0.0    0.0   0.0\n",
       "52  2.0  2.0  2.0  2.0  2.0  2.0    2.0   2.0\n",
       "53  2.0  2.0  2.0  2.0  2.0  2.0    2.0   2.0\n",
       "54  2.0  1.0  1.0  1.0  1.0  1.0    2.0   1.0\n",
       "55  5.0  5.0  5.0  5.0  5.0  5.0    5.0   5.0\n",
       "56  1.0  2.0  2.0  1.0  1.0  1.0    1.0   1.0\n",
       "57  3.0  2.0  3.0  3.0  3.0  3.0    3.0   3.0\n",
       "58  1.0  2.0  1.0  1.0  1.0  1.0    1.0   1.0\n",
       "59  2.0  2.0  2.0  2.0  2.0  2.0    2.0   2.0"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dup.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "2560e01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gbc</th>\n",
       "      <th>adb</th>\n",
       "      <th>xgb</th>\n",
       "      <th>sc</th>\n",
       "      <th>rf</th>\n",
       "      <th>mm</th>\n",
       "      <th>label</th>\n",
       "      <th>mode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     gbc  adb  xgb   sc   rf   mm  label  mode\n",
       "21   2.0  1.0  2.0  2.0  2.0  2.0    1.0   2.0\n",
       "29   3.0  3.0  3.0  3.0  2.0  3.0    2.0   3.0\n",
       "45   2.0  1.0  2.0  2.0  2.0  2.0    1.0   2.0\n",
       "54   2.0  1.0  1.0  1.0  1.0  1.0    2.0   1.0\n",
       "60   2.0  1.0  2.0  2.0  2.0  2.0    1.0   2.0\n",
       "160  0.0  0.0  1.0  0.0  0.0  1.0    1.0   0.0\n",
       "164  0.0  2.0  0.0  0.0  1.0  1.0    1.0   0.0\n",
       "174  1.0  1.0  1.0  1.0  1.0  1.0    0.0   1.0\n",
       "186  0.0  1.0  1.0  0.0  0.0  1.0    1.0   0.0\n",
       "232  1.0  1.0  1.0  1.0  1.0  1.0    0.0   1.0\n",
       "236  2.0  1.0  1.0  2.0  2.0  2.0    1.0   2.0\n",
       "288  2.0  2.0  2.0  2.0  2.0  2.0    0.0   2.0\n",
       "293  1.0  2.0  2.0  1.0  1.0  1.0    2.0   1.0\n",
       "299  2.0  2.0  2.0  2.0  2.0  2.0    3.0   2.0\n",
       "330  2.0  2.0  0.0  2.0  2.0  2.0    1.0   2.0\n",
       "346  0.0  2.0  0.0  0.0  2.0  2.0    1.0   0.0\n",
       "347  2.0  2.0  2.0  2.0  2.0  2.0    1.0   2.0\n",
       "375  2.0  2.0  2.0  2.0  2.0  2.0    4.0   2.0\n",
       "386  2.0  2.0  2.0  2.0  2.0  2.0    1.0   2.0\n",
       "387  2.0  1.0  1.0  2.0  1.0  1.0    2.0   1.0\n",
       "398  2.0  1.0  1.0  2.0  2.0  2.0    1.0   2.0\n",
       "405  0.0  1.0  1.0  0.0  0.0  1.0    1.0   0.0\n",
       "417  2.0  2.0  1.0  1.0  2.0  2.0    1.0   2.0\n",
       "448  2.0  2.0  1.0  2.0  2.0  2.0    1.0   2.0\n",
       "456  1.0  1.0  1.0  1.0  2.0  2.0    2.0   1.0\n",
       "459  2.0  1.0  2.0  2.0  2.0  2.0    1.0   2.0\n",
       "479  2.0  2.0  2.0  2.0  2.0  2.0    3.0   2.0\n",
       "493  2.0  2.0  2.0  2.0  2.0  2.0    1.0   2.0\n",
       "502  0.0  0.0  0.0  0.0  0.0  0.0    1.0   0.0\n",
       "507  0.0  0.0  0.0  0.0  0.0  0.0    1.0   0.0\n",
       "509  1.0  0.0  2.0  1.0  1.0  1.0    2.0   1.0\n",
       "580  1.0  2.0  1.0  1.0  2.0  2.0    2.0   1.0\n",
       "593  2.0  2.0  2.0  2.0  2.0  2.0    1.0   2.0\n",
       "613  2.0  2.0  2.0  2.0  2.0  2.0    1.0   2.0\n",
       "618  4.0  5.0  5.0  4.0  4.0  5.0    5.0   4.0\n",
       "626  2.0  2.0  2.0  2.0  2.0  2.0    3.0   2.0\n",
       "635  3.0  3.0  2.0  3.0  3.0  2.0    2.0   3.0\n",
       "647  1.0  1.0  1.0  1.0  1.0  1.0    2.0   1.0\n",
       "656  2.0  3.0  3.0  2.0  2.0  3.0    3.0   2.0\n",
       "689  1.0  1.0  1.0  2.0  1.0  1.0    3.0   1.0\n",
       "703  2.0  2.0  2.0  2.0  2.0  2.0    3.0   2.0\n",
       "724  2.0  1.0  2.0  2.0  2.0  2.0    1.0   2.0\n",
       "756  0.0  0.0  0.0  0.0  0.0  0.0    1.0   0.0\n",
       "767  1.0  2.0  1.0  1.0  2.0  2.0    2.0   1.0\n",
       "773  2.0  2.0  2.0  2.0  2.0  2.0    0.0   2.0\n",
       "774  0.0  0.0  1.0  0.0  1.0  1.0    1.0   0.0\n",
       "783  2.0  2.0  2.0  2.0  2.0  2.0    1.0   2.0\n",
       "801  2.0  2.0  2.0  2.0  2.0  2.0    1.0   2.0\n",
       "825  0.0  0.0  0.0  0.0  0.0  0.0    1.0   0.0\n",
       "875  1.0  2.0  1.0  1.0  1.0  1.0    2.0   1.0"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_rows = new_dup[new_dup[\"label\"] != new_dup[\"mode\"]]\n",
    "\n",
    "# Display the result\n",
    "filtered_rows.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a22dd2a",
   "metadata": {},
   "source": [
    "# Clustring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "836cc323",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-7be6684a5f89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mdbscan_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdbscan_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mkmeans_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmeans_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mmean_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[0moptics_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptics_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mgaussian_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgaussian_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_predict\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;31m# non-optimized default implementation; override when a better\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m         \u001b[1;31m# method is possible for a given clustering algorithm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\cluster\\_mean_shift.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    400\u001b[0m         all_res = Parallel(n_jobs=self.n_jobs)(\n\u001b[0;32m    401\u001b[0m             \u001b[0mdelayed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_mean_shift_single_seed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m             (seed, X, nbrs, self.max_iter) for seed in seeds)\n\u001b[0m\u001b[0;32m    403\u001b[0m         \u001b[1;31m# copy results in a dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseeds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    862\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 864\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    865\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 782\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    783\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 264\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 264\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\cluster\\_mean_shift.py\u001b[0m in \u001b[0;36m_mean_shift_single_seed\u001b[1;34m(my_mean, X, nbrs, max_iter)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;31m# Find mean of points within bandwidth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         i_nbrs = nbrs.radius_neighbors([my_mean], bandwidth,\n\u001b[1;32m---> 98\u001b[1;33m                                        return_distance=False)[0]\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[0mpoints_within\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi_nbrs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoints_within\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\neighbors\\_base.py\u001b[0m in \u001b[0;36mradius_neighbors\u001b[1;34m(self, X, radius, return_distance, sort_results)\u001b[0m\n\u001b[0;32m   1020\u001b[0m                 \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneigh_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneigh_ind\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m                 \u001b[0mneigh_ind_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunked_results\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m                 \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_to_object_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneigh_ind_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[1;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[0;32m   1622\u001b[0m             \u001b[0mX_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1623\u001b[0m         D_chunk = pairwise_distances(X_chunk, Y, metric=metric,\n\u001b[1;32m-> 1624\u001b[1;33m                                      n_jobs=n_jobs, **kwds)\n\u001b[0m\u001b[0;32m   1625\u001b[0m         if ((X is Y or Y is None)\n\u001b[0;32m   1626\u001b[0m                 \u001b[1;32mand\u001b[0m \u001b[0mPAIRWISE_DISTANCE_FUNCTIONS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[1;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[0;32m   1788\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1790\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1791\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1359\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m     \u001b[1;31m# enforce a threading backend to prevent data communication overhead\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[1;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# if dtype is already float64, no need to chunk and upcast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m         \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mYY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from numpy import unique\n",
    "from numpy import where\n",
    "from matplotlib import pyplot\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# initialize the data set we'll work with\n",
    "training_data = x_train\n",
    "\n",
    "# define the models\n",
    "dbscan_model = DBSCAN(eps=0.25, min_samples=6)\n",
    "kmeans_model = KMeans(n_clusters=6)\n",
    "mean_model = MeanShift()\n",
    "optics_model = OPTICS(eps=0.75, min_samples=6)\n",
    "gaussian_model = GaussianMixture(n_components=6)\n",
    "\n",
    "# train the model\n",
    "kmeans_model.fit(training_data)\n",
    "gaussian_model.fit(training_data)\n",
    "\n",
    "# assign each data point to a cluster\n",
    "dbscan_result = dbscan_model.fit_predict(training_data)\n",
    "kmeans_result = kmeans_model.predict(training_data)\n",
    "mean_result = mean_model.fit_predict(training_data)\n",
    "optics_result = optics_model.fit_predict(training_data)\n",
    "gaussian_result = gaussian_model.predict(training_data)\n",
    "\n",
    "# get all of the unique clusters\n",
    "dbscan_clusters = unique(dbscan_result)\n",
    "kmeans_clusters = unique(kmeans_result)\n",
    "mean_clusters = unique(mean_result)\n",
    "optics_clusters = unique(optics_result)\n",
    "gaussian_clusters = unique(gaussian_result)\n",
    "\n",
    "# show the DBSCAN plot\n",
    "pyplot.show()\n",
    "\n",
    "# plot the DBSCAN clusters\n",
    "for dbscan_cluster in dbscan_clusters:\n",
    "    # get data points that fall in this cluster\n",
    "    index = where(dbscan_result == dbscan_clusters)\n",
    "    # make the plot\n",
    "    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n",
    "\n",
    "# show the DBSCAN plot\n",
    "pyplot.show()\n",
    "\n",
    "# plot the KMeans clusters\n",
    "for kmeans_cluster in kmeans_clusters:\n",
    "    # get data points that fall in this cluster\n",
    "    index = where(kmeans_result == kmeans_clusters)\n",
    "    # make the plot\n",
    "    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n",
    "\n",
    "# show the KMeans plot\n",
    "pyplot.show()\n",
    "\n",
    "# plot Mean-Shift the clusters\n",
    "for mean_cluster in mean_clusters:\n",
    "    # get data points that fall in this cluster\n",
    "    index = where(mean_result == mean_cluster)\n",
    "    # make the plot\n",
    "    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n",
    "\n",
    "# show the Mean-Shift plot\n",
    "pyplot.show()\n",
    "\n",
    "# plot OPTICS the clusters\n",
    "for optics_cluster in optics_clusters:\n",
    "    # get data points that fall in this cluster\n",
    "    index = where(optics_result == optics_clusters)\n",
    "    # make the plot\n",
    "    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n",
    "\n",
    "# show the OPTICS plot\n",
    "pyplot.show()\n",
    "\n",
    "# plot Gaussian Mixture the clusters\n",
    "for gaussian_cluster in gaussian_clusters:\n",
    "    # get data points that fall in this cluster\n",
    "    index = where(gaussian_result == gaussian_clusters)\n",
    "    # make the plot\n",
    "    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n",
    "\n",
    "# show the Gaussian Mixture plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb2edd",
   "metadata": {},
   "source": [
    "# Clustring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e17a406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PATNO</th>\n",
       "      <th>PDMEDYN</th>\n",
       "      <th>DBSYN</th>\n",
       "      <th>NP3SPCH</th>\n",
       "      <th>NP3FACXP</th>\n",
       "      <th>NP3RIGN</th>\n",
       "      <th>NP3RIGRU</th>\n",
       "      <th>NP3RIGLU</th>\n",
       "      <th>NP3RIGRL</th>\n",
       "      <th>NP3RIGLL</th>\n",
       "      <th>...</th>\n",
       "      <th>NP3RTCON</th>\n",
       "      <th>NP3TOT</th>\n",
       "      <th>DYSKPRES</th>\n",
       "      <th>PAG_NAME_NUPDR3OF</th>\n",
       "      <th>PAG_NAME_NUPDR3ON</th>\n",
       "      <th>PAG_NAME_NUPDRDOSE3</th>\n",
       "      <th>PAG_NAME_NUPDRS3</th>\n",
       "      <th>PAG_NAME_NUPDRS3A</th>\n",
       "      <th>Cluster k</th>\n",
       "      <th>NHY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5233</th>\n",
       "      <td>42457</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4763</th>\n",
       "      <td>3278</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7387</th>\n",
       "      <td>75505</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8527</th>\n",
       "      <td>42346</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5855</th>\n",
       "      <td>41412</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10297</th>\n",
       "      <td>3453</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8220</th>\n",
       "      <td>145939</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8437</th>\n",
       "      <td>3407</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6048</th>\n",
       "      <td>52518</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10062</th>\n",
       "      <td>3854</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5886</th>\n",
       "      <td>101476</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8034</th>\n",
       "      <td>101477</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>3392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6708</th>\n",
       "      <td>3421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9154</th>\n",
       "      <td>74977</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>3469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4800</th>\n",
       "      <td>56680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7471</th>\n",
       "      <td>3203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10887</th>\n",
       "      <td>50184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7877</th>\n",
       "      <td>54991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PATNO  PDMEDYN  DBSYN  NP3SPCH  NP3FACXP  NP3RIGN  NP3RIGRU  NP3RIGLU  \\\n",
       "5233    42457      0.0    0.0      0.0       0.0      1.0       1.0       1.0   \n",
       "4763     3278      1.0    0.0      1.0       2.0      1.0       2.0       1.0   \n",
       "7387    75505      1.0    1.0      1.0       1.0      2.0       2.0       1.0   \n",
       "8527    42346      1.0    0.0      0.0       1.0      0.0       0.0       0.0   \n",
       "5855    41412      1.0    0.0      0.0       1.0      0.0       1.0       0.0   \n",
       "10297    3453      1.0    0.0      0.0       0.0      0.0       0.0       0.0   \n",
       "8220   145939      1.0    0.0      2.0       2.0      1.0       2.0       2.0   \n",
       "8437     3407      1.0    0.0      1.0       3.0      2.0       2.0       3.0   \n",
       "6048    52518      0.0    0.0      1.0       0.0      0.0       0.0       0.0   \n",
       "10062    3854      1.0    0.0      0.0       0.0      0.0       1.0       0.0   \n",
       "5886   101476      1.0    0.0      1.0       1.0      1.0       2.0       0.0   \n",
       "8034   101477      0.0    0.0      1.0       1.0      1.0       2.0       2.0   \n",
       "676      3392      0.0    0.0      0.0       1.0      1.0       0.0       2.0   \n",
       "6708     3421      0.0    0.0      0.0       1.0      2.0       1.0       1.0   \n",
       "9154    74977      1.0    0.0      0.0       1.0      0.0       0.0       0.0   \n",
       "1182     3469      0.0    0.0      1.0       1.0      0.0       0.0       0.0   \n",
       "4800    56680      0.0    0.0      0.0       0.0      0.0       0.0       0.0   \n",
       "7471     3203      1.0    0.0      0.0       2.0      1.0       0.0       1.0   \n",
       "10887   50184      0.0    0.0      0.0       0.0      0.0       0.0       0.0   \n",
       "7877    54991      0.0    0.0      1.0       0.0      0.0       0.0       0.0   \n",
       "\n",
       "       NP3RIGRL  NP3RIGLL  ...  NP3RTCON  NP3TOT  DYSKPRES  PAG_NAME_NUPDR3OF  \\\n",
       "5233        0.0       0.0  ...       0.0     3.0       0.0                  0   \n",
       "4763        1.0       1.0  ...       4.0    40.0       0.0                  0   \n",
       "7387        2.0       1.0  ...       4.0    57.0       0.0                  0   \n",
       "8527        0.0       0.0  ...       0.0    16.0       0.0                  0   \n",
       "5855        0.0       0.0  ...       0.0    17.0       1.0                  0   \n",
       "10297       0.0       0.0  ...       0.0     1.0       0.0                  0   \n",
       "8220        0.0       0.0  ...       0.0    32.0       0.0                  0   \n",
       "8437        2.0       3.0  ...       3.0    53.0       0.0                  1   \n",
       "6048        0.0       0.0  ...       0.0     5.0       0.0                  0   \n",
       "10062       0.0       0.0  ...       0.0     1.0       0.0                  0   \n",
       "5886        1.0       0.0  ...       1.0    15.0       0.0                  0   \n",
       "8034        1.0       0.0  ...       2.0    22.0       0.0                  0   \n",
       "676         2.0       1.0  ...       2.0    17.0       0.0                  0   \n",
       "6708        1.0       0.0  ...       2.0    18.0       0.0                  0   \n",
       "9154        0.0       0.0  ...       1.0    16.0       0.0                  0   \n",
       "1182        1.0       0.0  ...       1.0    16.0       0.0                  0   \n",
       "4800        0.0       0.0  ...       0.0     1.0       0.0                  0   \n",
       "7471        0.0       2.0  ...       1.0    24.0       0.0                  0   \n",
       "10887       0.0       0.0  ...       0.0     0.0       0.0                  0   \n",
       "7877        0.0       0.0  ...       0.0     3.0       0.0                  0   \n",
       "\n",
       "       PAG_NAME_NUPDR3ON  PAG_NAME_NUPDRDOSE3  PAG_NAME_NUPDRS3  \\\n",
       "5233                   0                    0                 1   \n",
       "4763                   0                    1                 0   \n",
       "7387                   0                    1                 0   \n",
       "8527                   0                    1                 0   \n",
       "5855                   0                    0                 1   \n",
       "10297                  0                    0                 1   \n",
       "8220                   0                    1                 0   \n",
       "8437                   0                    0                 0   \n",
       "6048                   0                    0                 1   \n",
       "10062                  0                    0                 1   \n",
       "5886                   0                    1                 0   \n",
       "8034                   0                    1                 0   \n",
       "676                    0                    0                 1   \n",
       "6708                   0                    0                 1   \n",
       "9154                   0                    1                 0   \n",
       "1182                   0                    0                 1   \n",
       "4800                   0                    1                 0   \n",
       "7471                   0                    0                 0   \n",
       "10887                  0                    0                 1   \n",
       "7877                   0                    0                 1   \n",
       "\n",
       "       PAG_NAME_NUPDRS3A  Cluster k  NHY  \n",
       "5233                   0          1  0.0  \n",
       "4763                   0          2  2.0  \n",
       "7387                   0          4  2.0  \n",
       "8527                   0          1  2.0  \n",
       "5855                   0          1  2.0  \n",
       "10297                  0          2  0.0  \n",
       "8220                   0          0  2.0  \n",
       "8437                   0          2  2.0  \n",
       "6048                   0          1  0.0  \n",
       "10062                  0          2  0.0  \n",
       "5886                   0          5  1.0  \n",
       "8034                   0          5  2.0  \n",
       "676                    0          2  2.0  \n",
       "6708                   0          2  2.0  \n",
       "9154                   0          4  1.0  \n",
       "1182                   0          2  1.0  \n",
       "4800                   0          4  0.0  \n",
       "7471                   1          2  2.0  \n",
       "10887                  0          1  0.0  \n",
       "7877                   0          4  0.0  \n",
       "\n",
       "[20 rows x 45 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans_model = KMeans(n_clusters=6)\n",
    "clusters = kmeans_model.fit_predict(x_train)\n",
    "new_x = x_train\n",
    "new_x['Cluster k'] = clusters\n",
    "new_x['NHY'] = y_train\n",
    "new_x.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61e3a41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Cluster k', ylabel='NHY'>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.scatterplot(data = new_x, x = 'Cluster k', y = 'NHY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcebfc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f2d5470c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generating sample data\n",
    "\n",
    "# Applying DBSCAN\n",
    "dbscan = DBSCAN(eps=0.1, min_samples=6)\n",
    "labels = dbscan.fit_predict(x_train)\n",
    "\n",
    "unique_labels = np.unique(labels)\n",
    "if -1 in unique_labels:\n",
    "    unique_labels = unique_labels[unique_labels != -1]  # Exclude noise label -1\n",
    "\n",
    "label_mapping = {label: i for i, label in enumerate(unique_labels)}\n",
    "mapped_labels = np.array([label_mapping.get(label, -1) for label in labels])\n",
    "\n",
    "# Plotting the results\n",
    "plt.scatter(x_train['NP3TOT'], x_train.iloc[:, 5], c=labels, cmap='viridis')\n",
    "plt.title('DBSCAN Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d7f9575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = new_x.drop('dbscan',axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e4c920fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PATNO</th>\n",
       "      <th>PDMEDYN</th>\n",
       "      <th>DBSYN</th>\n",
       "      <th>NP3SPCH</th>\n",
       "      <th>NP3FACXP</th>\n",
       "      <th>NP3RIGN</th>\n",
       "      <th>NP3RIGRU</th>\n",
       "      <th>NP3RIGLU</th>\n",
       "      <th>NP3RIGRL</th>\n",
       "      <th>NP3RIGLL</th>\n",
       "      <th>...</th>\n",
       "      <th>NP3RTCON</th>\n",
       "      <th>NP3TOT</th>\n",
       "      <th>DYSKPRES</th>\n",
       "      <th>PAG_NAME_NUPDR3OF</th>\n",
       "      <th>PAG_NAME_NUPDR3ON</th>\n",
       "      <th>PAG_NAME_NUPDRDOSE3</th>\n",
       "      <th>PAG_NAME_NUPDRS3</th>\n",
       "      <th>PAG_NAME_NUPDRS3A</th>\n",
       "      <th>Cluster k</th>\n",
       "      <th>NHY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5233</th>\n",
       "      <td>42457</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4763</th>\n",
       "      <td>3278</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7387</th>\n",
       "      <td>75505</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8527</th>\n",
       "      <td>42346</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5855</th>\n",
       "      <td>41412</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>52787</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7684</th>\n",
       "      <td>3905</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9845</th>\n",
       "      <td>3166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10799</th>\n",
       "      <td>71903</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>4085</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21536 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PATNO  PDMEDYN  DBSYN  NP3SPCH  NP3FACXP  NP3RIGN  NP3RIGRU  NP3RIGLU  \\\n",
       "5233   42457      0.0    0.0      0.0       0.0      1.0       1.0       1.0   \n",
       "4763    3278      1.0    0.0      1.0       2.0      1.0       2.0       1.0   \n",
       "7387   75505      1.0    1.0      1.0       1.0      2.0       2.0       1.0   \n",
       "8527   42346      1.0    0.0      0.0       1.0      0.0       0.0       0.0   \n",
       "5855   41412      1.0    0.0      0.0       1.0      0.0       1.0       0.0   \n",
       "...      ...      ...    ...      ...       ...      ...       ...       ...   \n",
       "1159   52787      0.0    0.0      0.0       0.0      0.0       0.0       1.0   \n",
       "7684    3905      1.0    0.0      0.0       1.0      1.0       1.0       1.0   \n",
       "9845    3166      0.0    0.0      1.0       1.0      2.0       3.0       3.0   \n",
       "10799  71903      0.0    0.0      0.0       0.0      0.0       0.0       0.0   \n",
       "2732    4085      1.0    0.0      0.0       0.0      0.0       0.0       0.0   \n",
       "\n",
       "       NP3RIGRL  NP3RIGLL  ...  NP3RTCON  NP3TOT  DYSKPRES  PAG_NAME_NUPDR3OF  \\\n",
       "5233        0.0       0.0  ...       0.0     3.0       0.0                  0   \n",
       "4763        1.0       1.0  ...       4.0    40.0       0.0                  0   \n",
       "7387        2.0       1.0  ...       4.0    57.0       0.0                  0   \n",
       "8527        0.0       0.0  ...       0.0    16.0       0.0                  0   \n",
       "5855        0.0       0.0  ...       0.0    17.0       1.0                  0   \n",
       "...         ...       ...  ...       ...     ...       ...                ...   \n",
       "1159        1.0       0.0  ...       0.0     6.0       0.0                  0   \n",
       "7684        1.0       1.0  ...       4.0    23.0       0.0                  0   \n",
       "9845        3.0       3.0  ...       0.0    40.0       0.0                  0   \n",
       "10799       0.0       0.0  ...       0.0    16.0       0.0                  0   \n",
       "2732        0.0       0.0  ...       0.0     0.0       0.0                  0   \n",
       "\n",
       "       PAG_NAME_NUPDR3ON  PAG_NAME_NUPDRDOSE3  PAG_NAME_NUPDRS3  \\\n",
       "5233                   0                    0                 1   \n",
       "4763                   0                    1                 0   \n",
       "7387                   0                    1                 0   \n",
       "8527                   0                    1                 0   \n",
       "5855                   0                    0                 1   \n",
       "...                  ...                  ...               ...   \n",
       "1159                   0                    0                 1   \n",
       "7684                   0                    0                 0   \n",
       "9845                   0                    0                 1   \n",
       "10799                  0                    1                 0   \n",
       "2732                   0                    0                 1   \n",
       "\n",
       "       PAG_NAME_NUPDRS3A  Cluster k  NHY  \n",
       "5233                   0          2  0.0  \n",
       "4763                   0          0  2.0  \n",
       "7387                   0          4  2.0  \n",
       "8527                   0          2  2.0  \n",
       "5855                   0          2  2.0  \n",
       "...                  ...        ...  ...  \n",
       "1159                   0          2  0.0  \n",
       "7684                   1          0  2.0  \n",
       "9845                   0          0  2.0  \n",
       "10799                  0          4  0.0  \n",
       "2732                   0          0  0.0  \n",
       "\n",
       "[21536 rows x 45 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6aea0f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33204866270430905"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(new_x['Cluster k'],new_x['NHY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ff0ce2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    10270\n",
       "1     4781\n",
       "4     3372\n",
       "5     1364\n",
       "0     1271\n",
       "3      478\n",
       "Name: Cluster k, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x['Cluster k'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "abcff9bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    10064\n",
       "0.0     7349\n",
       "1.0     3039\n",
       "3.0      875\n",
       "4.0      154\n",
       "5.0       55\n",
       "Name: NHY, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x['NHY'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c08dbb6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    5993\n",
       "1    2048\n",
       "4     718\n",
       "5     675\n",
       "0     481\n",
       "3     149\n",
       "Name: Cluster k, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x.loc[new_x['NHY'] == 2.0, 'Cluster k'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2932b4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
